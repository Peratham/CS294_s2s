{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf, re, time, math\n",
    "from wmt_data import *\n",
    "from seq2seq_model import *\n",
    "\n",
    "def create_model(V_en, V_fr, batch_size, buckets):\n",
    "    state_size = 256; num_layers = 1\n",
    "    max_gradient_norm = 5.0; batch_size = 128\n",
    "    learning_rate = 0.5; num_samples = 512\n",
    "    forward_only = True\n",
    "    return Seq2SeqModel(V_en, V_fr, buckets, state_size, num_layers, max_gradient_norm, batch_size, learning_rate, num_samples,\n",
    "      forward_only, dtype=tf.float32)\n",
    "\n",
    "V_en = 50000; V_fr = 50000; batch_size = 128\n",
    "max_train_data_size = 0\n",
    "buckets = [(5,10), (20,30)]\n",
    "en_train, fr_train, en_dev, fr_dev, en_vocab_path, fr_vocab_path = prepare_wmt_data(V_en, V_fr)\n",
    "\n",
    "dev_set = read_data(en_dev, fr_dev, buckets)\n",
    "train_set = read_data(en_train, fr_train, buckets, max_train_data_size)\n",
    "\n",
    "model = create_model(V_en, V_fr, batch_size, buckets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        steps_per_checkpoint = 200\n",
    "\n",
    "        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(buckets))]\n",
    "        train_total_size = float(sum(train_bucket_sizes))\n",
    "\n",
    "        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size for i in xrange(len(train_bucket_sizes))]\n",
    "\n",
    "        # This is the training loop.\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        current_step = 0\n",
    "        previous_losses = []\n",
    "        while True:\n",
    "            # Choose a bucket according to data distribution. We pick a random number\n",
    "            # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "            random_number_01 = np.random.random_sample() # Smart\n",
    "            bucket_id = min([i for i in xrange(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "            # Get a batch and make a step.\n",
    "            start_time = time.time()\n",
    "            encoder_inputs, decoder_inputs, target_weights = get_batch(train_set, batch_size, buckets, bucket_id)\n",
    "            _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "            step_time += (time.time() - start_time) / steps_per_checkpoint\n",
    "            loss += step_loss / steps_per_checkpoint\n",
    "            current_step += 1\n",
    "\n",
    "            # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "            if current_step % steps_per_checkpoint == 0:\n",
    "                # Print statistics for the previous epoch.\n",
    "                perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "                print (\"global step %d learning rate %.4f step-time %.2f perplexity %.2f\" % (model.global_step.eval(), model.learning_rate, step_time, perplexity))\n",
    "                # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "#                 if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "#                     sess.run(model.learning_rate_decay_op)\n",
    "                previous_losses.append(loss)\n",
    "                # Save checkpoint and zero timer and loss.\n",
    "                checkpoint_path = os.path.join(\"wmt/translate.ckpt\")\n",
    "                model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "                step_time, loss = 0.0, 0.0\n",
    "                # Run evals on development set and print their perplexity.\n",
    "                for bucket_id in xrange(len(buckets)):\n",
    "                    if len(dev_set[bucket_id]) == 0:\n",
    "                        print(\"  eval: empty bucket %d\" % (bucket_id))\n",
    "                        continue\n",
    "                    encoder_inputs, decoder_inputs, target_weights = get_batch(dev_set, batch_size, buckets, bucket_id)\n",
    "                    _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "                    eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\"inf\")\n",
    "                    print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n",
    "                sys.stdout.flush()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "_UNK .   \n",
      "_UNK .  .   .   \n",
      "------------------------------\n",
      "Visit unique museums  \n",
      "_UNK         \n",
      "------------------------------\n",
      "_UNK .   \n",
      "_UNK .  .   .   \n",
      "------------------------------\n",
      "A disappointment .  \n",
      "A .  .  .  .  \n",
      "------------------------------\n",
      "My _UNK identity crisis \n",
      "_UNK _UNK _UNK       \n",
      "------------------------------\n",
      "This is palliative care , given when there is nothing else that can be done .    \n",
      "Il est donc de façon que les changements de la façon de la façon de façon de façon de façon .  .   .    \n",
      "------------------------------\n",
      "With _UNK , pay and sell without banks            \n",
      "_UNK , _UNK et _UNK , _UNK                       \n",
      "------------------------------\n",
      "Many of the foreigners assert that receiving official status in our country is not that easy .   \n",
      "En 0000 , les pays qui ne sont pas de plus de plus en plus de plus de plus en plus de plus en plus .  .  \n",
      "------------------------------\n",
      "The buyer and seller often find each other through friends .         \n",
      "Les personnes et les autres groupes de travail sont très _UNK .  .  .              \n",
      "------------------------------\n",
      "Later , at a local pub , I _UNK attempted to _UNK .       \n",
      "En outre , le _UNK a été _UNK à _UNK .  .  .  .  .  .         \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# See some translations\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "vocab_en, rev_vocab_en = initialize_vocabulary(en_vocab_path)\n",
    "vocab_fr, rev_vocab_fr = initialize_vocabulary(fr_vocab_path)\n",
    "\n",
    "def buildSentence(ids, vocab):\n",
    "    sent = \" \".join([vocab[i] for i in ids])\n",
    "    return sent.replace('_PAD', '').replace('_EOS', '')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess, \"wmt/translate.ckpt-41000\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    for bucket_id in xrange(len(buckets)):\n",
    "        encoder_inputs, decoder_inputs, target_weights = get_batch(dev_set, batch_size, buckets, bucket_id)\n",
    "        _, loss, outputs = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "        english_sentences = np.array(encoder_inputs).T\n",
    "        french_sentences = np.argmax(np.array(outputs), axis=2).T\n",
    "        for i in range(5):\n",
    "            print buildSentence(reversed(english_sentences[i]), rev_vocab_en)\n",
    "            print buildSentence(french_sentences[i], rev_vocab_fr)\n",
    "            print \"------------------------------\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  reading data line 100000\n",
      "  reading data line 200000\n"
     ]
    }
   ],
   "source": [
    "# We can load data in batches ... :)\n",
    "vocab_en, rev_vocab_en = initialize_vocabulary(en_vocab_path)\n",
    "vocab_fr, rev_vocab_fr = initialize_vocabulary(fr_vocab_path)\n",
    "\n",
    "batch_size = 10\n",
    "buckets = [(20,30)]\n",
    "data = read_data(en_train_ids_path, fr_train_ids_path, buckets, 200000) # In buckets\n",
    "\n",
    "batch_encoder_inputs, batch_decoder_inputs, batch_weights = get_batch(data, batch_size, buckets, 0)\n",
    "\n",
    "# for j in range(batch_size):\n",
    "#     sent = [rev_vocab_en[i] for i in reversed([w[j] for w in batch_encoder_inputs])]\n",
    "#     sent_fr = [rev_vocab_fr[i] for i in [w[j] for w in batch_decoder_inputs]]\n",
    "#     print \" \".join(sent)\n",
    "#     print \" \".join(sent_fr[1:])\n",
    "#     print \"-------------------------------\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
